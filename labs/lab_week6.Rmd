---
title: 'Topic 6: Topic Analysis'
author: "Joe DeCesaro"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

```{r packages}
library(here)
library(pdftools)
library(quanteda)
library(tm)
library(topicmodels)
library(ldatuning)
library(tidyverse)
library(tidytext)
library(reshape2)

```

Load the data

```{r data}
##Topic 6 .Rmd here:https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/topic_6.Rmd
#grab data here: 
comments_df<-read_csv("https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/dat/comments_df.csv")

#comments_df <- read_csv(here("dat", "comments_df.csv")) #if reading from local
```

Now we'll build and clean the corpus

```{r corpus}
epa_corp <- corpus(x = comments_df, text_field = "text")
epa_corp.stats <- summary(epa_corp)
head(epa_corp.stats, n = 25)
toks <- tokens(epa_corp, remove_punct = TRUE, remove_numbers = TRUE)
#I added some project-specific stop words here
add_stops <- c(stopwords("en"),"environmental", "justice", "ej", "epa", "public", "comment")
toks1 <- tokens_select(toks, pattern = add_stops, selection = "remove")

```

And now convert to a document-feature matrix

```{r dfm}
dfm_comm<- dfm(toks1, tolower = TRUE)
dfm <- dfm_wordstem(dfm_comm)
dfm <- dfm_trim(dfm, min_docfreq = 2) #remove terms only appearing in one doc (min_termfreq = 10)

print(head(dfm))

#remove rows (docs) with all zeros
sel_idx <- slam::row_sums(dfm) > 0 
dfm <- dfm[sel_idx, ]
#comments_df <- dfm[sel_idx, ]
```

```{r }
#
result <- FindTopicsNumber(
  dfm,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)
```


```{r }
FindTopicsNumber_plot(result)
```

## Assignment:

Run three more models and select the overall best value for k (the number of topics) - include some justification for your selection: theory, FindTopicsNumber() optimization metrics, interpretability, LDAvis

### Model 1 (k = 10)

```{r}
k <- 10

topicModel_k10 <- LDA(dfm, k, 
                     method = "Gibbs", 
                     control = list(iter = 500, verbose = 25))

tmResult <- posterior(topicModel_k10)
terms(topicModel_k10, 10)
theta <- tmResult$topics
beta <- tmResult$terms # probability of each term in each topic
vocab <- (colnames(beta))
```

```{r}
comment_topics <- tidy(topicModel_k10, matrix = "beta")

top_terms <- comment_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

```{r }
top5termsPerTopic <- terms(topicModel_k10, 5)
topicNames_k10 <- apply(top5termsPerTopic, 2, paste, collapse=" ")
```

### Model 2 (k = 5)

```{r}
k <- 5

topicModel_k5 <- LDA(dfm, k, 
                     method = "Gibbs", 
                     control = list(iter = 500, verbose = 25))

tmResult <- posterior(topicModel_k5)
terms(topicModel_k5, 10)
theta <- tmResult$topics
beta <- tmResult$terms # probability of each term in each topic
vocab <- (colnames(beta))
```

```{r}
comment_topics <- tidy(topicModel_k5, matrix = "beta")

top_terms <- comment_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

### Model 3 (k = 14)

```{r}
k <- 14

topicModel_k14 <- LDA(dfm, k, 
                     method = "Gibbs", 
                     control = list(iter = 500, verbose = 25))

tmResult <- posterior(topicModel_k14)
terms(topicModel_k14, 10)
theta <- tmResult$topics
beta <- tmResult$terms # probability of each term in each topic
vocab <- (colnames(beta))
```

```{r}
comment_topics <- tidy(topicModel_k14, matrix = "beta")

top_terms <- comment_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms 

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

### Best k-value

The Deveaud method shows noticeable spikes at 5, 10, and 14 topics so I chose to explore these beyond the 7 and 9 topics we did in class. I based my choices on the Deveaud method because it does not just improve with the more topics you choose. 

The model where k=14 seems to be over fit as there are low beta values and multiple overlapping top words like communiti and state. For k=5 4/5 of the topics have the top word as communiti with low distinctiveness. When k=10 there is still low beta values however there only 4/10 categories have the same top word. To me this seems to mean we get more distinctive groups without spliting things up too much so I would go with k=10 as the best metric of the three k values I chose.