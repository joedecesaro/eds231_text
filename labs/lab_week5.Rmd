---
title: 'Topic 5: Word Relationships'
author: "Joe DeCesaro"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

```{r packages}
library(tidyr) #text analysis in R
library(pdftools)
library(lubridate) #working with date data
library(tidyverse)
library(tidytext)
library(readr)
library(quanteda)
library(readtext) #quanteda subpackage for reading pdf
library(quanteda.textstats)
library(quanteda.textplots)
library(ggplot2)
library(forcats)
library(stringr)
library(quanteda.textplots)
library(widyr)# pairwise correlations
library(igraph) #network plots
library(ggraph)
library(here)

```

#import EPA EJ Data

```{r pdf_import}
files <- list.files(path = here("data/"),
                    pattern = "EPA*",
                    full.names = TRUE)

ej_reports <- lapply(X = files, 
                     FUN = pdf_text)

ej_pdf <- readtext(file = files, 
                   docvarsfrom = "filenames", 
                   docvarnames = c("type", "year"),
                   sep = "_")

#creating an initial corpus containing our data
epa_corp <- corpus(x = ej_pdf, text_field = "text" )

#I'm adding some additional, context-specific stop words to stop word lexicon
more_stops <-c("2015","2016", "2017", "2018", "2019", "2020", "www.epa.gov", "https")
add_stops<- tibble(word = c(stop_words$word, more_stops)) 
stop_vec <- as_vector(add_stops)

```

Now we'll create some different data objects that will set us up for the subsequent analyses

```{r tidy}

#convert to tidy format and apply my stop words
raw_text <- tidy(epa_corp)

#Distribution of most frequent words across documents
raw_words <- raw_text %>%
  mutate(year = as.factor(year)) %>%
  unnest_tokens(word, text) %>%
  anti_join(add_stops, by = 'word') %>%
  count(year, word, sort = TRUE)

#number of total words by document  
total_words <- raw_words %>% 
  group_by(year) %>% 
  summarize(total = sum(n))

report_words <- left_join(raw_words, total_words)
 
par_tokens <- unnest_tokens(raw_text, output = paragraphs, input = text, token = "paragraphs")

par_tokens <- par_tokens %>%
 mutate(par_id = 1:n())

par_words <- unnest_tokens(par_tokens, output = word, input = paragraphs, token = "words")

```


```{r corr_paragraphs}

word_cors <- par_words %>% 
  add_count(par_id) %>% 
  filter(n >= 50) %>% 
  select(-n) %>%
  pairwise_cor(word, par_id, sort = TRUE)

```

Not surprisingly, the correlation between "environmental" and "justice" is by far the highest, which makes sense given the nature of these reports. How might we visualize these correlations to develop of sense of the context in which justice is discussed here?


```{r quanteda_init}
tokens <- tokens(epa_corp, remove_punct = TRUE)
toks1<- tokens_select(tokens, min_nchar = 3)
toks1 <- tokens_tolower(toks1)
toks1 <- tokens_remove(toks1, pattern = (stop_vec))
dfm <- dfm(toks1)

#first the basic frequency stat
tstat_freq <- textstat_frequency(dfm, n = 5, groups = year)
head(tstat_freq, 10)

```

Another useful word relationship concept is that of the n-gram, which essentially means tokenizing at the multi-word level. uni-gram vs. n-gram

```{r convert_dfm}
toks2 <- tokens_ngrams(toks1, n=2)
dfm2 <- dfm(toks2)
dfm2 <- dfm_remove(dfm2, pattern = c(stop_vec))
freq_words2 <- textstat_frequency(dfm2, n=20)
freq_words2$token <- rep("bigram", 20)
#tokens1 <- tokens_select(tokens1,pattern = stopwords("en"), selection = "remove")
```

Assignment

1.  What are the most frequent trigrams in the dataset? How does this compare to the most frequent bigrams? Which n-gram seems more informative here, and why?

```{r}
toks3 <- tokens_ngrams(toks1, n=3)
dfm3 <- dfm(toks3)
dfm3 <- dfm_remove(dfm3, pattern = c(stop_vec))
freq_words3 <- textstat_frequency(dfm3, n=20)
freq_words3$token <- rep("trigram")

```

```{r}
freq_words2 <- data.frame(freq_words2) %>% select(feature, frequency, token)
freq_words3 <- data.frame(freq_words3) %>% select(feature, frequency, token)

head(freq_words2)
head(freq_words3)
```

\noindent The trigram most frequent "features" seem to mostly be official names of reports and does not seem to tell us anything more interesting about the documents than the bigrams. I think the bigrams are a more useful tool.

2.  Choose a new focal term to replace "justice" and recreate the correlation table and network (see corr_paragraphs and corr_network chunks). Explore some of the plotting parameters in the cor_network chunk to see if you can improve the clarity or amount of information your plot conveys. Make sure to use a different color for the ties!

```{r}
just_cors <- word_cors %>% 
  filter(item1 == "indigenous")

word_cors %>%
  filter(item1 %in% c("indigenous"))%>%
  group_by(item1) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(item1 = as.factor(item1),
         name = reorder_within(item2, correlation, item1)) %>%
  ggplot(aes(y = name, x = correlation, fill = item1)) + 
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(y = NULL,
       x = NULL,
       title = "Correlations with key words",
       subtitle = "EPA EJ Reports")

```

```{r}
#let's zoom in on just one of our key terms
indigenous_cors <- word_cors %>% 
  filter(item1 == "indigenous") %>%
  mutate(n = 1:n())

indigenous_cors  %>%
  filter(n <= 30) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation, 
                     edge_width = correlation), 
                 edge_colour = "green4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), 
                 repel = TRUE, 
                 point.padding = unit(0.2, 
                                      "lines")) +
  theme_void()
```

3.  Write a function that allows you to conduct a keyness analysis to compare two individual EPA reports (hint: that means target and reference need to both be individual reports). Run the function on 3 pairs of reports, generating 3 keyness plots.

```{r}

#Distribution of most frequent words across documents
raw_words <- raw_text %>%
  mutate(year = as.factor(year)) %>%
  unnest_tokens(word, text) %>%
  anti_join(add_stops, by = 'word') %>%
  count(year, word, sort = TRUE)

```


```{r}
key_func <- function(target_rep, reference_rep){
  # read in the files
  files <- c(target_rep, reference_rep)
  reports <- lapply(X = files, 
                    FUN = pdf_text)
  
  pdf_files <- readtext(file = files, 
                        docvarsfrom = "filenames", 
                        docvarnames = c("type", "year"),
                        sep = "_")
  
  #creating an initial corpus containing our data
  corp <- corpus(x = pdf_files, text_field = "text" )
  
  # tokenize
  tokens <- tokens(corp, remove_punct = TRUE)
  toks1<- tokens_select(tokens, min_nchar = 3)
  toks1 <- tokens_tolower(toks1)
  toks1 <- tokens_remove(toks1, pattern = (stop_vec))
  dfm <- dfm(toks1)
  
  #first the basic frequency stat
  tstat_freq <- textstat_frequency(dfm, n = 5, groups = year)

  keyness <- textstat_keyness(dfm, target = 1)
  return(keyness)

}
```

```{r}
keyness1 <- key_func(target_rep = here("data", "EPAEJ_2016.pdf"),
                     reference_rep = here("data", "EPAEJ_2017.pdf"))

textplot_keyness(keyness1)
```

```{r}
keyness2 <- key_func(target_rep = here("data", "EPAEJ_2016.pdf"),
                     reference_rep = here("data", "EPAEJ_2019.pdf"))

textplot_keyness(keyness2)
```

```{r}
keyness3 <- key_func(target_rep = here("data", "EPAEJ_2016.pdf"),
                     reference_rep = here("data", "EPAEJ_2020.pdf"))

textplot_keyness(keyness3)
```

4.  Select a word or multi-word term of interest and identify words related to it using windowing and keyness comparison. To do this you will create two objects: one containing all words occurring within a 10-word window of your term of interest, and the second object containing all other words. Then run a keyness comparison on these objects. Which one is the target, and which the reference? [Hint](https://tutorials.quanteda.io/advanced-operations/target-word-collocations/)

```{r}
indigenous <- c("indigenous", "indigenous peoples", "peoples")
toks_inside <- tokens_keep(tokens, pattern = indigenous, window = 10)
toks_inside <- tokens_remove(toks_inside, pattern = indigenous) # remove the keywords
toks_outside <- tokens_remove(tokens, pattern = indigenous, window = 10)
```

```{r}
dfmat_inside <- dfm(toks_inside)
dfmat_outside <- dfm(toks_outside)

tstat_key_inside <- textstat_keyness(rbind(dfmat_inside, dfmat_outside), 
                                     target = seq_len(ndoc(dfmat_inside)))
head(tstat_key_inside, 20)
```

\noindent The tokens inside the 10-word window are the targets while the tokens outside the 10-word window are the references. The word "recognized" happens inside the 10 word window of indigenous words 15 times and outside of the window 13 times.


